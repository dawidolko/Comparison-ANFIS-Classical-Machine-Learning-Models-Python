# Jak dzia≈Ça projekt ANFIS - Kompletny przewodnik

## Czƒô≈õƒá 1: Co robi setup.sh od poczƒÖtku do ko≈Ñca

Kiedy uruchamiasz `./setup.sh`, wykonuje siƒô 7 krok√≥w w okre≈õlonej kolejno≈õci. Oto dok≈Çadnie co siƒô dzieje:

### KROK 1: Przygotowanie danych (data_preprocessing.py)

Skrypt bierze surowe pliki CSV i przygotowuje je do treningu modeli.

**Dla wina:**

- Wczytuje `winequality-red.csv` i `winequality-white.csv`
- ≈ÅƒÖczy je w jeden zbi√≥r ALBO u≈ºywa osobno (3 warianty: all, red, white)
- Zamienia kolumnƒô `quality` (liczby 3-9) na warto≈õci binarne: quality > 5 ‚Üí 1 (dobre wino), reszta ‚Üí 0 (s≈Çabe wino)
- Normalizuje wszystkie 11 cech (fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, alcohol) u≈ºywajƒÖc StandardScaler - ka≈ºda cecha ma ≈õredniƒÖ=0 i odchylenie=1
- Dzieli dane na train (80%) i test (20%)
- Zapisuje jako pliki `.npy`: `X_train_all.npy`, `y_train_all.npy`, `X_test_all.npy`, `y_test_all.npy` (podobnie dla red i white)
- Zapisuje scaler do `scaler_all.pkl` ≈ºeby m√≥c p√≥≈∫niej odwr√≥ciƒá normalizacjƒô

**Dla betonu:**

- Wczytuje `Concrete_Data.csv`
- 8 cech wej≈õciowych (cement, slag, fly ash, water, superplasticizer, coarse aggregate, fine aggregate, age)
- 1 cel: concrete compressive strength (wytrzyma≈Ço≈õƒá w MPa) - WA≈ªNE: to jest regresja, nie klasyfikacja!
- Normalizacja StandardScaler
- Podzia≈Ç 80/20
- Zapisuje do `concrete-strength/X_train.npy`, `y_train.npy`, `X_test.npy`, `y_test.npy`

Po tym kroku masz gotowe dane w formacie numpy arrays, znormalizowane i podzielone.

---

### KROK 2: Trening ANFIS (train_anfis.py)

Teraz zaczyna siƒô w≈Ça≈õciwy trening modeli ANFIS. Skrypt trenuje 8 r√≥≈ºnych modeli:

**6 modeli dla wina:**

1. `all_2memb` - wszystkie wina, 2 funkcje przynale≈ºno≈õci na cechƒô
2. `all_3memb` - wszystkie wina, 3 funkcje przynale≈ºno≈õci na cechƒô
3. `red_2memb` - tylko czerwone wina, 2 funkcje
4. `red_3memb` - tylko czerwone wina, 3 funkcje
5. `white_2memb` - tylko bia≈Çe wina, 2 funkcje
6. `white_3memb` - tylko bia≈Çe wina, 3 funkcje

**2 modele dla betonu:** 7. `concrete_2memb` - beton, 2 funkcje przynale≈ºno≈õci 8. `concrete_3memb` - beton, 3 funkcje przynale≈ºno≈õci

Dla ka≈ºdego modelu skrypt:

**A) Tworzy model ANFIS:**

```python
model = ANFISModel(n_input=11, n_memb=2, regression=False)  # Wine
# LUB
model = ANFISModel(n_input=8, n_memb=3, regression=True)  # Concrete
```

**B) Kompiluje go:**

- Wine (klasyfikacja): loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy']
- Concrete (regresja): loss='mse', optimizer='nadam', metrics=['mae']

**C) Trenuje przez max 20 epok:**

- Ka≈ºda epoka: model przetwarza wszystkie batche (32 pr√≥bki naraz)
- Walidacja na zbiorze testowym co epokƒô
- Early stopping: je≈õli val_loss nie poprawia siƒô przez 10 epok ‚Üí stop
- Model checkpoint: zapisuje najlepsze wagi do `models/anfis_all_best_2memb.weights.h5`

**D) Po treningu zapisuje:**

- `results/anfis_all_2memb_results.json` - metryki (train/test accuracy, loss, MAE, R¬≤)
- `results/anfis_all_2memb_rules.json` - wyekstrahowane regu≈Çy rozmyte (wiƒôcej o tym p√≥≈∫niej)
- `results/anfis_all_2memb_training.png` - wykresy krzywych uczenia (accuracy vs epoki, loss vs epoki)
- `results/anfis_all_2memb_fit.png` - scatter plot: prawdziwe warto≈õci vs predykcje

**E) Cross-validation (5-fold):**

- Dzieli dane treningowe na 5 czƒô≈õci
- 5 razy: trenuje na 4 czƒô≈õciach, testuje na 1
- Zapisuje `results/anfis_all_2memb_cv.json` - ≈õrednie i odchylenie standardowe metryk

Ca≈Çy ten proces zajmuje 15-30 minut w zale≈ºno≈õci od procesora.

---

### KROK 3: Wizualizacja funkcji przynale≈ºno≈õci (visualize_membership_functions.py)

Dla ka≈ºdego wytrenowanego modelu ANFIS skrypt tworzy wykresy pokazujƒÖce jak wyglƒÖdajƒÖ funkcje Gaussa.

Model ma wyuczone parametry `c` (centrum) i `sigma` (szeroko≈õƒá) dla ka≈ºdej funkcji przynale≈ºno≈õci. Skrypt:

- Bierze warto≈õci minimalne i maksymalne ka≈ºdej cechy ze zbioru treningowego
- Generuje 200 punkt√≥w w tym zakresie
- Dla ka≈ºdego punktu oblicza warto≈õƒá funkcji Gaussa: `exp(-(x - c)¬≤ / (2*sigma¬≤))`
- Rysuje wykresy dla wybranych cech (np. dla wina: alcohol, pH, sulphates; dla betonu: cement, water, age)

Zapisuje jako `results/membership_functions_all_2memb.png` (subplot z kilkoma cechami).

To pozwala zobaczyƒá jak model "widzi" dane - jakie zakresy warto≈õci uwa≈ºa za "niskie", "≈õrednie", "wysokie".

---

### KROK 4: Trening modeli por√≥wnawczych (train_comparison_models.py)

Teraz trenujemy 3 klasyczne modele ML do por√≥wnania z ANFIS.

**Dla wina (klasyfikacja):**

1. **Neural Network:**

   - Architektura: Input(11) ‚Üí Dense(16, relu) ‚Üí Dropout(0.3) ‚Üí Dense(8, relu) ‚Üí Dropout(0.2) ‚Üí Dense(1, sigmoid)
   - Adam optimizer, learning rate=0.001
   - Early stopping po 15 epokach bez poprawy
   - Class weights bo dane sƒÖ niezbalansowane (wiƒôcej dobrych win)
   - Zapisuje: `models/nn_wine.keras`, `results/nn_wine_results.json`

2. **SVM (Support Vector Machine):**

   - RBF kernel (Radial Basis Function)
   - C=1.0 (parametr regularyzacji)
   - gamma='scale' (szeroko≈õƒá kernela)
   - Class weights
   - Zapisuje: `results/svm_wine_results.json`

3. **Random Forest:**
   - 200 drzew decyzyjnych
   - max_depth=15 (maksymalna g≈Çƒôboko≈õƒá drzewa)
   - min_samples_split=5
   - Class weights
   - Zapisuje: `results/rf_wine_results.json`

**Dla betonu (regresja):**

1. **Neural Network:**

   - Input(8) ‚Üí Dense(32, relu) ‚Üí Dropout(0.3) ‚Üí Dense(16, relu) ‚Üí Dropout(0.2) ‚Üí Dense(1, linear)
   - Loss: MSE, metryka: MAE
   - Zapisuje: `models/nn_concrete.keras`, `results/nn_concrete_results.json`

2. **SVM Regressor:**

   - RBF kernel
   - C=1.0, epsilon=0.1
   - Zapisuje: `results/svm_concrete_results.json`

3. **Random Forest Regressor:**
   - 200 drzew, max_depth=15
   - Zapisuje: `results/rf_concrete_results.json`

Ka≈ºdy model zapisuje metryki: train accuracy/MAE, test accuracy/MAE, czas treningu.

---

### KROK 5: Eksploracja danych (data_exploration.py)

Tworzy wykresy analizy danych (EDA - Exploratory Data Analysis).

**Dla wina:**

- `wine_class_distribution.png` - wykres s≈Çupkowy: ile pr√≥bek ma quality=0 vs quality=1
- `wine_correlation.png` - macierz korelacji (heatmap) pokazujƒÖca zale≈ºno≈õci miƒôdzy cechami
- `wine_feature_distributions.png` - histogramy wszystkich 11 cech
- `wine_pairplot.png` - scatter plots par cech (np. alcohol vs pH)

**Dla betonu:**

- `concrete_strength_distribution.png` - histogram wytrzyma≈Ço≈õci betonu
- `concrete_correlation.png` - macierz korelacji 8 cech
- `concrete_feature_distributions.png` - histogramy cech
- `concrete_strength_vs_features.png` - scatter plots: cement vs strength, water vs strength, age vs strength

Wszystko zapisane w `results/`.

---

### KROK 6: Por√≥wnanie wszystkich modeli (compare_all_models.py)

Wczytuje wszystkie pliki `*_results.json` i tworzy por√≥wnawcze wykresy.

**Dla wina:**

- `model_comparison_bar_wine.png` - wykres s≈Çupkowy por√≥wnujƒÖcy accuracy wszystkich modeli (ANFIS 2MF, ANFIS 3MF, NN, SVM, RF)
- `overfitting_analysis_wine.png` - wykres train accuracy vs test accuracy dla ka≈ºdego modelu (pokazuje czy model siƒô przeuczy≈Ç)

**Dla betonu:**

- `model_comparison_bar_concrete.png` - wykres s≈Çupkowy por√≥wnujƒÖcy MAE wszystkich modeli
- `overfitting_analysis_concrete.png` - train MAE vs test MAE

To pozwala ≈Çatwo zobaczyƒá kt√≥ry model dzia≈Ça najlepiej.

---

### KROK 7: Uruchomienie GUI (streamlit run app.py)

Na ko≈Ñcu setup.sh uruchamia aplikacjƒô webowƒÖ Streamlit w tle.

Otwiera siƒô przeglƒÖdarka na `http://localhost:8501` z 5 zak≈Çadkami:

- Home - opis projektu
- ANFIS Results - interaktywne przeglƒÖdanie wynik√≥w ANFIS
- Rules & History - regu≈Çy rozmyte i historia treningu
- Data Analysis - wykresy EDA
- Model Comparison - por√≥wnanie ANFIS vs NN/SVM/RF

Aplikacja dzia≈Ça dop√≥ki nie zamkniesz terminala lub nie wci≈õniesz Ctrl+C.

---

## Czƒô≈õƒá 2: Co to jest ANFIS i jak dzia≈Ça

ANFIS to skr√≥t od Adaptive Neuro-Fuzzy Inference System. To model ≈ÇƒÖczƒÖcy sieci neuronowe z logikƒÖ rozmytƒÖ.

### Dlaczego ANFIS?

Zwyk≈Ça sieƒá neuronowa to "czarna skrzynka" - nie wiesz dlaczego podjƒô≈Ça decyzjƒô.
ANFIS generuje REGU≈ÅY kt√≥re mo≈ºesz przeczytaƒá:

```
JE≈öLI alcohol jest WYSOKI (0.85) AND pH jest ≈öREDNI (0.60)
TO quality = 0.5 + 0.03*alcohol - 0.02*pH = 0.76 (prawdopodobie≈Ñstwo dobrego wina)
```

Taka regu≈Ça ma interpretacjƒô: "Wino z wysokim alkoholem i ≈õrednim pH bƒôdzie raczej dobre".

### Architektura ANFIS - 5 warstw

ANFIS to przep≈Çyw danych przez 5 warstw. Ka≈ºda robi co≈õ innego.

```
Wej≈õcie (x) ‚Üí [Warstwa 1] ‚Üí [Warstwa 2] ‚Üí [Warstwa 3] ‚Üí [Warstwa 4] ‚Üí [Warstwa 5] ‚Üí Wyj≈õcie (y)
             Fuzzyfikacja   Regu≈Çy       Normalizacja  Defuzzyfikacja  Agregacja
```

### **WARSTWA 1: Fuzzyfikacja (FuzzyLayer)** üå´Ô∏è

**Co robi:** Zamienia liczby ostre na "stopnie przynale≈ºno≈õci" do zbior√≥w rozmytych.

**Wz√≥r - Gaussowska funkcja przynale≈ºno≈õci:**

```
Œº·µ¢‚±º(x) = exp(-(x‚±º - c·µ¢‚±º)¬≤ / (2œÉ·µ¢‚±º¬≤))
```

Gdzie:

- `x‚±º` = warto≈õƒá j-tej cechy wej≈õciowej (np. alkohol = 12.5%)
- `c·µ¢‚±º` = **centrum** i-tej funkcji przynale≈ºno≈õci dla cechy j (UCZONY parametr!)
- `œÉ·µ¢‚±º` = **szeroko≈õƒá** funkcji (UCZONY parametr!)
- `Œº·µ¢‚±º` = stopie≈Ñ przynale≈ºno≈õci (0.0 do 1.0)

**Przyk≈Çad:**

```
Dla cechy "alkohol":
- Funkcja 1 (LOW):  c=10, œÉ=1.5 ‚Üí dla x=9  ‚Üí Œº=0.85 (wysoki stopie≈Ñ "LOW")
- Funkcja 2 (HIGH): c=14, œÉ=1.5 ‚Üí dla x=9  ‚Üí Œº=0.03 (niski stopie≈Ñ "HIGH")
```

**Wyj≈õcie:** Tensor kszta≈Çtu **(batch_size, n_memb, n_features)**  
Dla Wine: `(32, 2, 11)` - 2 funkcje przynale≈ºno≈õci dla ka≈ºdej z 11 cech

**Kod w `anfis.py`:**

```python
class FuzzyLayer(tf.keras.layers.Layer):
    def call(self, x):
        x = tf.expand_dims(x, axis=1)  # (B, n) ‚Üí (B, 1, n)
        z = (x - self.c) / (self.sigma + 1e-8)
        mu = tf.exp(-0.5 * tf.square(z))  # Gaussa
        return tf.clip_by_value(mu, 1e-8, 1.0)
```

---

### **WARSTWA 2: Tworzenie Regu≈Ç (RuleLayer)** üìú

**Co robi:** Tworzy WSZYSTKIE mo≈ºliwe kombinacje funkcji przynale≈ºno≈õci = regu≈Çy rozmyte.

**Wz√≥r - T-norma (AND) przez iloczyn:**

```
w‚Çñ = Œº‚ÇÅ‚Çñ‚ÇÅ √ó Œº‚ÇÇ‚Çñ‚ÇÇ √ó ... √ó Œº‚Çô‚Çñ‚Çô
```

Gdzie:

- `w‚Çñ` = si≈Ça k-tej regu≈Çy (0.0 do 1.0)
- `k = (k‚ÇÅ, k‚ÇÇ, ..., k‚Çô)` = kombinacja indeks√≥w funkcji przynale≈ºno≈õci

**Przyk≈Çad dla 2 cech √ó 2 MF:**

```
Regu≈Ça 1: Œº‚ÇÅ(LOW) √ó Œº‚ÇÇ(LOW)   = 0.85 √ó 0.90 = 0.765
Regu≈Ça 2: Œº‚ÇÅ(LOW) √ó Œº‚ÇÇ(HIGH)  = 0.85 √ó 0.10 = 0.085
Regu≈Ça 3: Œº‚ÇÅ(HIGH) √ó Œº‚ÇÇ(LOW)  = 0.15 √ó 0.90 = 0.135
Regu≈Ça 4: Œº‚ÇÅ(HIGH) √ó Œº‚ÇÇ(HIGH) = 0.15 √ó 0.10 = 0.015
```

**Liczba regu≈Ç:** `n_memb^n_features`

- Wine (11 cech, 2 MF): 2^11 = **2,048 regu≈Ç**
- Wine (11 cech, 3 MF): 3^11 = **177,147 regu≈Ç** üò±
- Concrete (8 cech, 3 MF): 3^8 = **6,561 regu≈Ç**

**Wyj≈õcie:** Tensor **(batch_size, n_rules)**  
Dla Wine 3MF: `(32, 177147)` - si≈Ça ka≈ºdej regu≈Çy

**Kod w `anfis.py`:**

```python
class RuleLayer(tf.keras.layers.Layer):
    def call(self, mu):
        out = mu[:, :, 0]  # Pierwsza cecha
        for i in range(1, self.n):
            out = tf.einsum("bm,bn->bmn", out, mu[:, :, i])  # Iloczyn
            out = tf.reshape(out, (tf.shape(mu)[0], -1))
        return out
```

---

### **WARSTWA 3: Normalizacja (NormLayer)** ‚öñÔ∏è

**Co robi:** Normalizuje si≈Çy regu≈Ç tak, aby sumowa≈Çy siƒô do 1.

**Wz√≥r:**

```
wÃÑ‚Çñ = w‚Çñ / (w‚ÇÅ + w‚ÇÇ + ... + w‚Çô)
```

**Przyk≈Çad:**

```
w = [0.765, 0.085, 0.135, 0.015]  ‚Üí suma = 1.0
wÃÑ = [0.765, 0.085, 0.135, 0.015] (ju≈º znormalizowane)
```

**Wyj≈õcie:** Tensor **(batch_size, n_rules)** - znormalizowane wagi

**Kod w `anfis.py`:**

```python
class NormLayer(tf.keras.layers.Layer):
    def call(self, w):
        s = tf.reduce_sum(w, axis=1, keepdims=True)
        return w / (s + 1e-8)
```

---

### **WARSTWA 4: Defuzzyfikacja (DefuzzLayer)** üéØ

**Co robi:** Oblicza **konsekwent** ka≈ºdej regu≈Çy (czƒô≈õƒá THEN) wed≈Çug modelu TSK-1.

**Wz√≥r konsekwentu k-tej regu≈Çy:**

```
f‚Çñ = w‚ÇÄ‚Çñ + w‚ÇÅ‚Çñx‚ÇÅ + w‚ÇÇ‚Çñx‚ÇÇ + ... + w‚Çô‚Çñx‚Çô
```

Gdzie:

- `w‚ÇÄ‚Çñ` = **bias** k-tej regu≈Çy (UCZONY parametr!)
- `w‚ÇÅ‚Çñ, w‚ÇÇ‚Çñ, ...` = **wagi** konsekwentu (UCZONE parametry!)

**Potem mno≈ºy przez znormalizowanƒÖ wagƒô:**

```
y‚Çñ = wÃÑ‚Çñ √ó f‚Çñ
```

**Przyk≈Çad:**

```
Regu≈Ça 1: f‚ÇÅ = 0.5 + 0.3√óalkohol - 0.1√ókwasowo≈õƒá = 0.5 + 0.3√ó12 - 0.1√ó5 = 4.1
         y‚ÇÅ = 0.765 √ó 4.1 = 3.14

Regu≈Ça 2: f‚ÇÇ = -0.2 + 0.5√óalkohol + 0.2√ókwasowo≈õƒá = -0.2 + 0.5√ó12 + 0.2√ó5 = 6.8
         y‚ÇÇ = 0.085 √ó 6.8 = 0.58
```

**Wyj≈õcie:** Tensor **(batch_size, n_rules)** - wk≈Çad ka≈ºdej regu≈Çy

**Kod w `anfis.py`:**

```python
class DefuzzLayer(tf.keras.layers.Layer):
    def call(self, w_norm, x):
        y = tf.matmul(x, self.CP_weight) + self.CP_bias  # Konsekwent TSK
        return w_norm * y  # Mno≈ºenie przez wagƒô regu≈Çy
```

---

### **WARSTWA 5: Agregacja (SummationLayer)** ‚ûï

**Co robi:** Sumuje wk≈Çady wszystkich regu≈Ç = ko≈Ñcowe wyj≈õcie ANFIS.

**Wz√≥r:**

```
y = Œ£‚Çñ y‚Çñ = Œ£‚Çñ (wÃÑ‚Çñ √ó f‚Çñ)
```

**Przyk≈Çad:**

```
y = 3.14 + 0.58 + 0.40 + 0.05 = 4.17
```

Dla **klasyfikacji** (Wine): `y` przechodzi przez **sigmoid** ‚Üí prawdopodobie≈Ñstwo (0-1)  
Dla **regresji** (Concrete): `y` pozostaje **linear** ‚Üí warto≈õƒá MPa (0-100)

**Wyj≈õcie:** Tensor **(batch_size, 1)** - predykcja ko≈Ñcowa

**Kod w `anfis.py`:**

```python
class SummationLayer(tf.keras.layers.Layer):
    def call(self, per_rule):
        return tf.reduce_sum(per_rule, axis=1, keepdims=True)
```

---

## üéì Jak dzia≈Ça trening?

### 1. **Forward pass** (przep≈Çyw w prz√≥d)

```
x ‚Üí FuzzyLayer ‚Üí RuleLayer ‚Üí NormLayer ‚Üí DefuzzLayer ‚Üí SummationLayer ‚Üí Activation ‚Üí ≈∑
```

### 2. **Obliczenie b≈Çƒôdu (Loss)**

**Dla Wine (klasyfikacja):**

```
Loss = Binary Cross-Entropy = -[y√ólog(≈∑) + (1-y)√ólog(1-≈∑)]
```

**Dla Concrete (regresja):**

```
Loss = MSE = (y - ≈∑)¬≤
MAE = |y - ≈∑|  (metry pomocnicza)
```

### 3. **Backward pass** (propagacja wsteczna)

TensorFlow automatycznie oblicza gradienty dla WSZYSTKICH parametr√≥w:

- Centra `c` i szeroko≈õci `œÉ` funkcji Gaussa (Warstwa 1)
- Wagi `w` i bias `b` konsekwent√≥w TSK (Warstwa 4)

### 4. **Aktualizacja parametr√≥w**

Optymalizator **Nadam** (Adam z Nesterov momentum):

```
Œ∏_new = Œ∏_old - learning_rate √ó gradient
```

Learning rate = **0.001** (sta≈Ça)

### 5. **Early Stopping**

Trening ko≈Ñczy siƒô gdy **val_loss** nie poprawia siƒô przez **10 epok**.

---

## üìä Przep≈Çyw danych w setup.sh

### **KROK 1: data_preprocessing.py**

```
CSV ‚Üí pandas ‚Üí binary labels ‚Üí StandardScaler ‚Üí train/test split ‚Üí .npy + .pkl
```

**Dla Wine:**

```python
quality ‚Üí (quality > 5).astype(int) ‚Üí y_binary
11 cech ‚Üí StandardScaler.fit_transform() ‚Üí X_normalized
```

**Dla Concrete:**

```python
8 cech ‚Üí StandardScaler.fit_transform() ‚Üí X_normalized
strength (MPa) ‚Üí y (bez zmian, regresja)
```

---

### **KROK 2: train_anfis.py**

#### **2.1 Tworzenie modelu**

```python
# Dla Wine (klasyfikacja)
model = ANFISModel(n_input=11, n_memb=2, regression=False)
model.compile(loss="binary_crossentropy", optimizer="nadam", metrics=["accuracy"])

# Dla Concrete (regresja)
model = ANFISModel(n_input=8, n_memb=3, regression=True)
model.compile(loss="mse", optimizer="nadam", metrics=["mae"])
```

#### **2.2 Trening**

```python
model.fit(X_train, y_train, validation_data=(X_test, y_test),
          epochs=20, callbacks=[ModelCheckpoint, EarlyStopping])
```

**Callbacks:**

- `ModelCheckpoint` - zapisuje najlepsze wagi do `models/anfis_*.weights.h5`
- `EarlyStopping` - zatrzymuje trening po 10 epokach bez poprawy

#### **2.3 Wizualizacja**

```python
plot_training_history()  # Krzywe accuracy/MAE + loss
plot_fit_on_train()      # Scatter plot y_true vs y_pred + R¬≤
```

#### **2.4 Ekstrakcja regu≈Ç**

```python
centers, sigmas = model.get_membership_functions()
weights, bias = model.weights, model.bias

# Dla ka≈ºdej regu≈Çy k:
rule_k = {
    "membership_indices": [k1, k2, ..., kn],  # Kt√≥re MF sƒÖ aktywne
    "consequent": {
        "weights": [w1k, w2k, ..., wnk],
        "bias": w0k
    }
}
```

**Zapis do `results/anfis_*_rules.json`**

---

### **KROK 3: visualize_membership_functions.py**

```python
# Dla ka≈ºdej cechy j:
x_range = np.linspace(X_min[j], X_max[j], 200)

for i in range(n_memb):
    mu = exp(-(x_range - c[i,j])¬≤ / (2*sigma[i,j]¬≤))
    plt.plot(x_range, mu, label=f"MF {i+1}")
```

**Wyj≈õcie:** `results/membership_functions_*.png` - wykresy Gaussa

---

### **KROK 4: train_comparison_models.py**

Trenuje 3 klasyczne modele:

**Neural Network:**

```
Input(11) ‚Üí Dense(16, relu) ‚Üí Dropout(0.3) ‚Üí Dense(8, relu) ‚Üí Dropout(0.2) ‚Üí Dense(1, sigmoid)
```

**SVM:**

```
RBF kernel, C=1.0, gamma='scale'
```

**Random Forest:**

```
200 drzew, max_depth=15
```

---

### **KROK 5: data_exploration.py**

Generuje wykresy EDA:

- Rozk≈Çad klas (`wine_class_distribution.png`)
- Macierz korelacji (`wine_correlation.png`)
- Histogramy cech (`wine_feature_distributions.png`)
- Pairplot (`wine_pairplot.png`)

---

### **KROK 6: compare_all_models.py**

Wczytuje wszystkie `*_results.json` i tworzy:

- `model_comparison_bar.png` - wykres s≈Çupkowy accuracy/MAE
- `overfitting_analysis.png` - train vs test gap

---

### **KROK 7: app.py (Streamlit)**

```python
streamlit run app.py
```

Uruchamia GUI na `http://localhost:8501` z 5 zak≈Çadkami.

---

## üî¢ Przyk≈Çad numeryczny ANFIS

### **Problem:** Przewidzieƒá jako≈õƒá wina na podstawie alkoholu i kwasowo≈õci

**Dane:**

```
x‚ÇÅ = alkohol = 12.0%
x‚ÇÇ = kwasowo≈õƒá = 5.0
y = jako≈õƒá = 1 (dobra)
```

**Model:** ANFIS z 2 funkcjami przynale≈ºno≈õci (LOW, HIGH)

---

### **WARSTWA 1: Fuzzyfikacja**

**Parametry (wyuczone):**

```
c‚ÇÅ‚ÇÅ=10, œÉ‚ÇÅ‚ÇÅ=2  (alkohol LOW)
c‚ÇÇ‚ÇÅ=14, œÉ‚ÇÇ‚ÇÅ=2  (alkohol HIGH)
c‚ÇÅ‚ÇÇ=4, œÉ‚ÇÅ‚ÇÇ=1   (kwasowo≈õƒá LOW)
c‚ÇÇ‚ÇÇ=7, œÉ‚ÇÇ‚ÇÇ=1   (kwasowo≈õƒá HIGH)
```

**Obliczenia:**

```
Œº‚ÇÅ‚ÇÅ = exp(-(12-10)¬≤/(2√ó2¬≤)) = exp(-0.5) = 0.606  (alkohol LOW)
Œº‚ÇÇ‚ÇÅ = exp(-(12-14)¬≤/(2√ó2¬≤)) = exp(-0.5) = 0.606  (alkohol HIGH)
Œº‚ÇÅ‚ÇÇ = exp(-(5-4)¬≤/(2√ó1¬≤))   = exp(-0.5) = 0.606  (kwasowo≈õƒá LOW)
Œº‚ÇÇ‚ÇÇ = exp(-(5-7)¬≤/(2√ó1¬≤))   = exp(-2.0) = 0.135  (kwasowo≈õƒá HIGH)
```

---

### **WARSTWA 2: Regu≈Çy**

**4 regu≈Çy (2√ó2):**

```
w‚ÇÅ = Œº‚ÇÅ‚ÇÅ √ó Œº‚ÇÅ‚ÇÇ = 0.606 √ó 0.606 = 0.367  (LOW alkohol AND LOW kwasowo≈õƒá)
w‚ÇÇ = Œº‚ÇÅ‚ÇÅ √ó Œº‚ÇÇ‚ÇÇ = 0.606 √ó 0.135 = 0.082  (LOW alkohol AND HIGH kwasowo≈õƒá)
w‚ÇÉ = Œº‚ÇÇ‚ÇÅ √ó Œº‚ÇÅ‚ÇÇ = 0.606 √ó 0.606 = 0.367  (HIGH alkohol AND LOW kwasowo≈õƒá)
w‚ÇÑ = Œº‚ÇÇ‚ÇÅ √ó Œº‚ÇÇ‚ÇÇ = 0.606 √ó 0.135 = 0.082  (HIGH alkohol AND HIGH kwasowo≈õƒá)
```

---

### **WARSTWA 3: Normalizacja**

```
suma = 0.367 + 0.082 + 0.367 + 0.082 = 0.898

wÃÑ‚ÇÅ = 0.367/0.898 = 0.409
wÃÑ‚ÇÇ = 0.082/0.898 = 0.091
wÃÑ‚ÇÉ = 0.367/0.898 = 0.409
wÃÑ‚ÇÑ = 0.082/0.898 = 0.091
```

---

### **WARSTWA 4: Defuzzyfikacja**

**Parametry konsekwent√≥w (wyuczone):**

```
Regu≈Ça 1: w‚ÇÄ=0.5, w‚ÇÅ=0.03, w‚ÇÇ=-0.05
Regu≈Ça 2: w‚ÇÄ=-0.2, w‚ÇÅ=0.02, w‚ÇÇ=0.08
Regu≈Ça 3: w‚ÇÄ=0.8, w‚ÇÅ=0.05, w‚ÇÇ=-0.03
Regu≈Ça 4: w‚ÇÄ=0.1, w‚ÇÅ=0.04, w‚ÇÇ=0.02
```

**Obliczenia konsekwent√≥w:**

```
f‚ÇÅ = 0.5 + 0.03√ó12 - 0.05√ó5 = 0.61
f‚ÇÇ = -0.2 + 0.02√ó12 + 0.08√ó5 = 0.44
f‚ÇÉ = 0.8 + 0.05√ó12 - 0.03√ó5 = 1.25
f‚ÇÑ = 0.1 + 0.04√ó12 + 0.02√ó5 = 0.68
```

**Wk≈Çady regu≈Ç:**

```
y‚ÇÅ = 0.409 √ó 0.61 = 0.249
y‚ÇÇ = 0.091 √ó 0.44 = 0.040
y‚ÇÉ = 0.409 √ó 1.25 = 0.511
y‚ÇÑ = 0.091 √ó 0.68 = 0.062
```

---

### **WARSTWA 5: Agregacja + Activation**

```
y_raw = 0.249 + 0.040 + 0.511 + 0.062 = 0.862

y_final = sigmoid(0.862) = 1/(1+e^(-0.862)) = 0.703
```

**Interpretacja:** Model przewiduje prawdopodobie≈Ñstwo **70.3%**, ≈ºe wino jest dobrej jako≈õci.

**Rzeczywista etykieta:** `y=1` ‚Üí wino jest dobre ‚úì

**Loss (Binary Cross-Entropy):**

```
Loss = -[1√ólog(0.703) + 0√ólog(0.297)] = -log(0.703) = 0.352
```

---

## üéØ Kluczowe wnioski

### 1. **ANFIS to "bia≈Ça skrzynka"**

- Mo≈ºesz zobaczyƒá, kt√≥re regu≈Çy sƒÖ aktywne
- Mo≈ºesz zinterpretowaƒá wagi konsekwent√≥w
- Przyk≈Çad: "Je≈õli alkohol wysoki i kwasowo≈õƒá niska ‚Üí jako≈õƒá dobra (waga 0.511)"

### 2. **Parametry uczƒÖ siƒô automatycznie**

- Centra i szeroko≈õci funkcji Gaussa
- Wagi i bias konsekwent√≥w TSK
- **Gradient descent** przez TensorFlow

### 3. **Liczba regu≈Ç ro≈õnie wyk≈Çadniczo**

- 2 MF, 11 cech ‚Üí 2,048 regu≈Ç
- 3 MF, 11 cech ‚Üí 177,147 regu≈Ç (!!)
- Dlatego u≈ºywamy **top-K** regu≈Ç w ekstrakcji

### 4. **ANFIS dzia≈Ça dobrze na ma≈Çych zbiorach**

- Wine: 5,197 pr√≥bek treningowych
- Concrete: 824 pr√≥bki
- NN/SVM/RF czƒôsto wymagajƒÖ wiƒôcej danych

---

## üìö Bibliografia

1. **Jang, J.-S. R. (1993)**. "ANFIS: Adaptive-Network-Based Fuzzy Inference System"  
   _IEEE Transactions on Systems, Man, and Cybernetics_, vol. 23, no. 3, pp. 665-685.

2. **Takagi, T., & Sugeno, M. (1985)**. "Fuzzy identification of systems and its applications to modeling and control"  
   _IEEE Transactions on Systems, Man, and Cybernetics_, vol. 15, no. 1, pp. 116-132.

---

## üîó Dodatkowe materia≈Çy

- **Kod ≈∫r√≥d≈Çowy ANFIS:** [`anfis.py`](anfis.py)
- **Skrypt treningu:** [`train_anfis.py`](train_anfis.py)
- **Dokumentacja widok√≥w GUI:** [`WIDOKI_APLIKACJI.md`](WIDOKI_APLIKACJI.md)
- **README projektu:** [`README.md`](README.md)
