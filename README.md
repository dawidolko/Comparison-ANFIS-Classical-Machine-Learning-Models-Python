# ANFIS-Wine-Quality-Classification  

> ğŸš€ **ANFIS vs Classical Machine Learning** - Comprehensive comparison of Adaptive Neuro-Fuzzy Inference System with traditional ML algorithms on real-world datasets

## ğŸ“‹ Description

Welcome to the **ANFIS Wine Quality Classification** repository! This comprehensive project compares ANFIS (Adaptive Neuro-Fuzzy Inference System) with classical machine learning algorithms on two real-world datasets: Wine Quality Classification and Concrete Compressive Strength Prediction. The system demonstrates the power of neuro-fuzzy systems by combining the learning capabilities of neural networks with the interpretability of fuzzy logic systems.

Built with TensorFlow/Keras and featuring automated pipeline execution, this project showcases best practices in fuzzy systems implementation, machine learning model comparison, and scientific experimentation. The system includes comprehensive data exploration, cross-validation, membership function visualization, and interactive Streamlit GUI for real-time predictions.

## ğŸ“ Repository Structure

```

ANFIS-Wine-Quality-Classification/
â”œâ”€â”€ ğŸ“ data/ # Raw datasets
â”‚ â”œâ”€â”€ ğŸ“ wine-quality/
â”‚ â”‚ â”œâ”€â”€ ğŸ· winequality-red.csv # Red wine dataset
â”‚ â”‚ â””â”€â”€ ğŸ· winequality-white.csv # White wine dataset
â”‚ â””â”€â”€ ğŸ“ concrete-strength/
â”‚ â””â”€â”€ ğŸ—ï¸ Concrete_Data.csv # Concrete strength dataset
â”œâ”€â”€ ğŸ“ models/ # Trained model weights (generated)
â”‚ â”œâ”€â”€ anfis_2memb.weights.h5
â”‚ â”œâ”€â”€ anfis_3memb.weights.h5
â”‚ â”œâ”€â”€ neural_network.pkl
â”‚ â”œâ”€â”€ svm_model.pkl
â”‚ â””â”€â”€ random_forest.pkl
â”œâ”€â”€ ğŸ“ results/ # Generated plots and metrics
â”‚ â”œâ”€â”€ ğŸ“Š all_models_comparison.png
â”‚ â”œâ”€â”€ ğŸ“ˆ overfitting_analysis.png
â”‚ â”œâ”€â”€ ğŸ§  anfis_2memb_training.png
â”‚ â”œâ”€â”€ ğŸ§  anfis_3memb_training.png
â”‚ â”œâ”€â”€ ğŸ“‰ membership_functions_visualization.png
â”‚ â”œâ”€â”€ ğŸ“Š quality_distribution.png
â”‚ â”œâ”€â”€ ğŸ”¥ correlation_matrix.png
â”‚ â””â”€â”€ \*.json (numerical results)
â”œâ”€â”€ ğŸ§  anfis.py # ANFIS core implementation
â”œâ”€â”€ ğŸ“Š data_exploration.py # Exploratory data analysis
â”œâ”€â”€ ğŸ”„ data_preprocessing.py # Data preparation and normalization
â”œâ”€â”€ ğŸ‹ï¸ train_anfis.py # ANFIS model training
â”œâ”€â”€ ğŸ¤– train_comparison_models.py # Train NN, SVM, Random Forest
â”œâ”€â”€ ğŸ“ˆ compare_all_models.py # Results comparison and visualization
â”œâ”€â”€ ğŸ“‰ visualize_membership_functions.py # Membership function plots
â”œâ”€â”€ ğŸ› ï¸ utils.py # Helper functions (NEW v1.1.0)
â”œâ”€â”€ ğŸ“ scaller.py # Scaler management (NEW v1.1.0)
â”œâ”€â”€ ğŸ¯ app.py # Streamlit web interface
â”œâ”€â”€ ğŸš€ main.py # Main automated pipeline
â”œâ”€â”€ ğŸ“‹ requirements.txt # Python dependencies
â”œâ”€â”€ ğŸ”§ setup.sh # Linux/macOS setup script
â”œâ”€â”€ ğŸ”§ setup.bat # Windows setup script
â”œâ”€â”€ ğŸ“– MANUAL_INSTRUCTION.md # Detailed installation guide
â””â”€â”€ ğŸ“– README.md # Project documentation

```

## ğŸš€ Quick Start

### Prerequisites

- **Python** 3.8-3.12 (tested on 3.12)
- **pip** package manager
- **4GB RAM** minimum
- **~1GB disk space** for dependencies and datasets

### One-Command Automated Setup

#### Linux/macOS:

```bash
chmod +x setup.sh
./setup.sh
```

#### Windows:

```bash
setup.bat
```

### What the Setup Script Does:

1. âœ… Creates virtual environment
2. âœ… Installs all dependencies
3. âœ… Downloads and preprocesses datasets
4. âœ… Trains ANFIS models (2 & 3 membership functions)
5. âœ… Performs 5-fold cross-validation
6. âœ… Visualizes membership functions
7. âœ… Generates data exploration plots
8. âœ… Trains comparison models (NN, SVM, RF)
9. âœ… Creates comparison charts
10. âœ… Launches Streamlit GUI at [http://localhost:8501](http://localhost:8501)

**â±ï¸ Estimated time:** 15-30 minutes (CPU-dependent)

## âš™ï¸ System Requirements

### **Essential Tools:**

- **Python** 3.8-3.12 (Python 3.13+ not compatible with TensorFlow 2.17)
- **pip** package manager
- **4GB RAM** minimum (8GB recommended)
- **1GB disk space** for dependencies

### **Required Python Libraries:**

```
tensorflow==2.17.0
numpy==1.26.4
pandas==2.2.3
scikit-learn==1.5.2
matplotlib==3.9.2
seaborn==0.12.2
streamlit==1.39.0
h5py==3.12.1
pillow==11.0.0
```

### **Manual Installation:**

```bash
# Install dependencies
pip install -r requirements.txt

# Run automated pipeline
python main.py

# Or run individual steps (see Manual Execution section)
```

### **Development Environment:**

- **Code Editor** (VS Code, PyCharm, Jupyter Notebook)
- **Python Debugger** for development
- **Git** for version control

## âœ¨ Key Features

### **ğŸ§  ANFIS Implementation**

- **5-Layer Takagi-Sugeno-Kang Architecture:**
  1. **Fuzzy Layer** - Gaussian membership functions with learned parameters
  2. **Rule Layer** - Fuzzy rule generation (T-norm multiplication)
  3. **Norm Layer** - Rule weight normalization
  4. **Defuzz Layer** - TSK-type defuzzification with linear consequents
  5. **Summation Layer** - Weighted output aggregation

- **Configurable Membership Functions:**
  - 2 membership functions: 2,048 fuzzy rules
  - 3 membership functions: 177,147 fuzzy rules

- **Advanced Training:**
  - Nadam optimizer (learning rate: 0.001)
  - Early stopping (patience: 10 epochs)
  - Model checkpointing (saves best weights)
  - 20 training epochs

### **ğŸ“Š Two Real-World Datasets**

#### **1. Wine Quality Classification ğŸ·**

- **Source:** [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/wine+quality)
- **Samples:** 6,497 (1,599 red + 4,898 white wines)
- **Features:** 11 physicochemical properties
  - Fixed acidity, volatile acidity, citric acid
  - Residual sugar, chlorides
  - Free/total sulfur dioxide
  - Density, pH, sulphates, alcohol
- **Task:** Binary classification (quality > 5 vs â‰¤ 5)
- **Variants:** Combined (all), red only, white only

#### **2. Concrete Compressive Strength ğŸ—ï¸**

- **Source:** [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength)
- **Samples:** 1,030
- **Features:** 8 concrete components
  - Cement, blast furnace slag, fly ash
  - Water, superplasticizer
  - Coarse/fine aggregate, age (days)
- **Task:** Regression (predict compressive strength in MPa)

### **ğŸ¤– Model Comparison**

| Model              | Type           | Configuration                       |
| ------------------ | -------------- | ----------------------------------- |
| **ANFIS**          | Neuro-Fuzzy    | 2 & 3 Gaussian membership functions |
| **Neural Network** | Deep Learning  | 16â†’Dropoutâ†’8â†’Dropoutâ†’1 architecture |
| **SVM**            | Kernel Methods | RBF kernel, C=1.0                   |
| **Random Forest**  | Ensemble       | 200 trees, max_depth=15             |

### **ğŸ“ˆ Comprehensive Evaluation**

- **Cross-Validation:** 5-fold stratified (classification) / standard (regression)
- **Metrics:** Accuracy, MAE, MSE, train-test gap
- **Overfitting Analysis:** Train-test performance comparison
- **Statistical Significance:** Multiple random seeds

### **ğŸ“Š Rich Visualizations**

- Training curves (accuracy/loss over epochs)
- Prediction scatter plots (predicted vs actual)
- Membership function plots for top features
- Correlation heatmaps
- Feature distribution histograms
- Model comparison bar charts
- Overfitting analysis plots
- Publication-ready 300 DPI PNG exports

### **ğŸ¯ Interactive GUI**

Streamlit web interface with:

- ğŸ  **Dashboard** - Project overview and statistics
- ğŸ“Š **Model Results** - Comparison and ranking tables
- ğŸ§  **ANFIS Theory** - Architecture explanations and visualizations
- ğŸ“ˆ **Data Exploration** - Dataset insights and distributions
- ğŸ· **Real-Time Prediction** - Interactive wine quality prediction

### **ğŸ”§ Modular Architecture**

- Separate modules for each functionality
- Clean separation of concerns (v1.1.0)
- Reusable utility functions
- Easy to extend and modify
- Well-documented code with Polish docstrings

## ğŸ› ï¸ Technologies Used

- **TensorFlow 2.17** - Deep learning framework
- **Keras** - High-level neural networks API
- **NumPy** - Numerical computing
- **Pandas** - Data manipulation and analysis
- **Scikit-learn** - Machine learning algorithms
- **Matplotlib** - Data visualization
- **Seaborn** - Statistical data visualization
- **Streamlit** - Interactive web applications
- **H5py** - HDF5 file format for model weights

## ğŸ“– Step-by-Step Manual Execution

### Step 1: Data Exploration ğŸ“Š

```bash
python data_exploration.py
```

**What it does:**

- Downloads Wine Quality dataset (red + white)
- Combines datasets (6,497 samples total)
- Analyzes quality distribution (scale 3-9)
- Checks for missing values and feature correlations
- Generates visualizations

**Output:**

- âœ… `quality_distribution.png` - Quality distribution histogram
- âœ… `correlation_matrix.png` - Feature correlation heatmap

### Step 2: Data Preprocessing ğŸ”„

```bash
python data_preprocessing.py
```

**What it does:**

- Transforms problem into binary classification:
  - **Class 0** (poor quality): quality â‰¤ 5
  - **Class 1** (good quality): quality > 5
- Selects 11 most important features
- Splits data: 80% training, 20% testing
- Applies StandardScaler normalization (critical for ANFIS!)
- Saves processed data to `.npy` files

**Output:**

- âœ… 5,197 training samples
- âœ… 1,300 test samples
- âœ… Class distribution: 2,384 poor / 4,113 good quality

### Step 3: ANFIS Training ğŸ§ 

```bash
python train_anfis.py
```

**What it does:**

- Trains 2 ANFIS models:
  - **ANFIS with 2 membership functions** (2,048 rules)
  - **ANFIS with 3 membership functions** (177,147 rules)
- Each model trains for 20 epochs
- Uses Nadam optimizer + binary cross-entropy
- Saves best model weights (ModelCheckpoint)
- Early stopping after 15 epochs without improvement
- Generates training plots for each model

**ANFIS Architecture:**

1. **FuzzyLayer** - Gaussian membership functions (Î¼(x) = exp(-(x-c)Â²/(2ÏƒÂ²)))
2. **RuleLayer** - Fuzzy rule generation (T-norm AND operation)
3. **NormLayer** - Rule weight normalization
4. **DefuzzLayer** - TSK defuzzification (linear combination)
5. **SummationLayer** - Output aggregation

**Results:**

- âœ… ANFIS (2 functions): Test Accuracy = **69.06%**
- âœ… ANFIS (3 functions): Test Accuracy = **76.48%**
- âœ… Models saved in `models/`
- âœ… Training plots in `results/`

**Execution time:** ~2 minutes

### Step 4: Comparison Models Training ğŸ¤–

```bash
python train_comparison_models.py
```

**What it does:**
Trains 3 classical machine learning models:

#### **Neural Network (NN)**

- Architecture: 16 â†’ Dropout(0.3) â†’ 8 â†’ Dropout(0.2) â†’ 1
- Activation functions: ReLU + Sigmoid
- Optimizer: Adam
- 50 epochs with early stopping

#### **Support Vector Machine (SVM)**

- Kernel: RBF (Radial Basis Function)
- Hyperparameters: C=1.0, gamma='scale'
- Trained on full dataset

#### **Random Forest**

- 200 decision trees
- max_depth=15
- Parallel training (n_jobs=-1)

**Results:**

- âœ… Neural Network: Test Accuracy = **75.69%**
- âœ… SVM: Test Accuracy = **77.85%**
- âœ… Random Forest: Test Accuracy = **83.23%** ğŸ†
- âœ… All models saved in `models/`

**Execution time:** ~5-10 minutes

### Step 5: Model Comparison ğŸ“ˆ

```bash
python compare_all_models.py
```

**What it does:**

- Loads results from all 5 models
- Generates 2 comparison plots:
  - **all_models_comparison.png** - Train vs Test bar chart
  - **overfitting_analysis.png** - Train-Test gap analysis
- Displays detailed ranking table

**Final Rankings:**

```
ğŸ¥‡ #1: Random Forest     - 83.23% (overfitting: 14.46% âš ï¸)
ğŸ¥ˆ #2: SVM               - 77.85% (minimal overfitting: 1.47%)
ğŸ¥‰ #3: ANFIS (3 functions)- 76.48% (slight overfitting: 4.59%)
   #4: Neural Network    - 75.69% (minimal overfitting: 1.76%)
   #5: ANFIS (2 functions)- 69.06% (no overfitting: 0.75%)
```

### Step 6: Membership Function Visualization ğŸ“‰

```bash
python visualize_membership_functions.py
```

**What it does:**

- Loads ANFIS model weights
- Plots Gaussian membership functions for 6 most important features
- Saves visualization to `membership_functions_visualization.png`

### Step 7: Launch Interactive GUI ğŸ¯

```bash
streamlit run app.py
```

**Features:**

- Real-time wine quality prediction
- Model comparison dashboard
- ANFIS architecture explanations
- Data exploration tools
- Interactive visualizations

Access at: [http://localhost:8501](http://localhost:8501)

## ğŸ“Š Results Analysis

### Final Model Comparison

| Ranking | Model          | Test Accuracy | Train Accuracy | Overfitting | Interpretability    |
| ------- | -------------- | ------------- | -------------- | ----------- | ------------------- |
| ğŸ¥‡      | Random Forest  | **83.23%**    | 97.69%         | 14.46% âš ï¸   | âŒ Black box        |
| ğŸ¥ˆ      | SVM            | **77.85%**    | 79.31%         | 1.47% âœ…    | âŒ Black box        |
| ğŸ¥‰      | ANFIS (3 MF)   | **76.48%**    | 81.08%         | 4.59% âœ…    | âœ… **Fuzzy rules!** |
| 4       | Neural Network | **75.69%**    | 77.45%         | 1.76% âœ…    | âŒ Black box        |
| 5       | ANFIS (2 MF)   | **69.06%**    | 69.81%         | 0.75% âœ…    | âœ… **Fuzzy rules!** |

### Key Insights

âœ… **ANFIS is Competitive!**

- ANFIS (3 functions) achieves 76.48% - only 6.75% below best model
- Better than classical Neural Network (75.69%)
- Minimal overfitting (4.59%)

âœ… **ANFIS Provides Interpretability!**

- Visualized membership functions show learned patterns
- Identifiable fuzzy rules (e.g., "IF alcohol HIGH AND acidity LOW THEN quality GOOD")
- Other models are "black boxes"

âš ï¸ **Random Forest Overfits**

- Highest test accuracy (83.23%)
- But severe overfitting (14.46%)
- Train accuracy = 97.69% (nearly perfect fit to training data)

ğŸ”¬ **3 Membership Functions >> 2 Membership Functions**

- +7.42% accuracy improvement (76.48% vs 69.06%)
- More rules = better data representation
- Computational trade-off: 2,048 rules vs 177,147 rules

## ğŸ”¬ Fuzzy Logic Elements in ANFIS

### Gaussian Membership Function

```
Î¼(x) = exp(-(x - c)Â² / (2ÏƒÂ²))
```

**Parameters (learned during training):**

- `c` - center of function
- `Ïƒ` - width/spread of function

### Fuzzy Rules Example

```
Rule 1: IF alcohol is HIGH AND acidity is LOW
        THEN quality is GOOD

Rule 2: IF alcohol is LOW AND acidity is HIGH
        THEN quality is POOR
```

### Takagi-Sugeno Defuzzification

```
Output = Î£(wáµ¢ Ã— (aáµ¢xâ‚ + báµ¢xâ‚‚ + ... + cáµ¢))
```

where `wáµ¢` are normalized rule weights

## ğŸ†• Version 1.1.0 Changes

### âœ… Optimizations Implemented

1. **ğŸ–¼ï¸ Fixed Matplotlib Blocking**
   - Added `matplotlib.use('Agg')` to all plotting scripts
   - Removed all `plt.show()` calls - plots save automatically
   - **Effect:** Pipeline executes without stopping for windows!

2. **ğŸ“¦ Business Logic Separation**
   - Created `utils.py` - ANFIS model and results loading functions
   - Created `scaller.py` - centralized scaler management
   - **Effect:** `app.py` contains only Streamlit UI code

3. **ğŸš« Extended .gitignore**
   - Ignores generated files (_.npy, _.h5, _.pkl, _.png)
   - **Effect:** Repository clean of binary artifacts

4. **ğŸ“š Complete Documentation**
   - `CHANGELOG.md` - detailed technical changes
   - `MANUAL_INSTRUCTION.md` - step-by-step installation guide
   - **Backward compatible:** All changes maintain compatibility âœ…

## ğŸ§° Troubleshooting

### Issue: Streamlit doesn't launch automatically

**Solution:** Manually run `streamlit run app.py` after setup completes

### Issue: TensorFlow installation fails

**Solution:**

- Ensure Python 3.8-3.12 (TensorFlow 2.17 not compatible with Python 3.13+)
- Try: `pip install tensorflow==2.17.0 --no-cache-dir`

### Issue: Out of memory during training

**Solution:** Reduce batch size in `train_anfis.py` (line 95: `batch_size=16`)

### Issue: Matplotlib backend errors

**Solution:**

- Install: `pip install python3-tk` (Linux)
- Or use backend: `export MPLBACKEND=Agg` before running scripts

### Issue: Dataset download fails

**Solution:** Manually download datasets from UCI ML Repository and place in `data/` directory

## ğŸ“ Conclusions

1. **ANFIS combines best of both worlds:**
   - Learning like neural networks
   - Interpretation like expert systems

2. **3 membership functions significantly better than 2:**
   - +7.42% accuracy (76.48% vs 69.06%)
   - More rules = better data representation

3. **ANFIS vs Classical Models:**
   - Random Forest best but overfits
   - SVM solid choice (77.85%, minimal overfitting)
   - **ANFIS excellent compromise:** good accuracy + interpretability

4. **Wine Quality Problem:**
   - 11 numerical features, 6,497 samples
   - Class imbalance (37% poor / 63% good quality)
   - All models achieve >75% accuracy

## ğŸ¤ Contributing

Contributions are highly welcomed! Here's how you can help:

- ğŸ› **Report bugs** - Found an issue? Let us know!
- ğŸ’¡ **Suggest improvements** - Have ideas for better features?
- ğŸ”§ **Submit pull requests** - Share your enhancements and solutions
- ğŸ“– **Improve documentation** - Help make the project clearer

Feel free to open issues or reach out through GitHub for any questions or suggestions.

## ğŸ‘¨â€ğŸ’» Authors

Created by:

- **Dawid Olko** - Project Lead
- **Piotr SmoÅ‚a** - ML Implementation
- **Jakub Opar** - Data Analysis
- **MichaÅ‚ Pilecki** - Visualization

Course: Fuzzy Systems  
Supervisor: mgr inÅ¼. Marcin Mrukowicz  
RzeszÃ³w University of Technology, 2025/2026

## ğŸ“„ License

This project is open source and available under the [MIT License](LICENSE).

---
